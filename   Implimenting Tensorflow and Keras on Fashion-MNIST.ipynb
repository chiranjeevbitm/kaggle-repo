{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3b2fb25e2ab812829bf74f624b40ed2f664e4636"
   },
   "source": [
    "## Data Description\n",
    "Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255. The training and test data sets have 785 columns. The first column consists of the class labels (see above), and represents the article of clothing. The rest of the columns contain the pixel-values of the associated image.\n",
    "### Labels\n",
    "Each training and test example is assigned to one of the following labels:\n",
    "\n",
    "0 T-shirt/top\n",
    "\n",
    "1 Trouser\n",
    "\n",
    "2 Pullover\n",
    "\n",
    "3 Dress\n",
    "\n",
    "4 Coat\n",
    "\n",
    "5 Sandal\n",
    "\n",
    "6 Shirt\n",
    "\n",
    "7 Sneaker\n",
    "\n",
    "8 Bag\n",
    "\n",
    "9 Ankle boot\n",
    "\n",
    "\n",
    "\n",
    "Each row is a separate image\n",
    "Column 1 is the class label.\n",
    "Remaining columns are pixel numbers (784 total).\n",
    "Each value is the darkness of the pixel (1 to 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6190cb0690947063098cc00f7cccd23bb07643dd"
   },
   "source": [
    "### Loading useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Input      data     files     are    available in the \"../input/\" directory.\n",
    "\n",
    "from subprocess import check_output\n",
    "\n",
    "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
    "\n",
    "# Import Pandas for data manipulation using dataframes\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#Import Numpy for statistical calculations\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Import Warnings \n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import matplotlib Library for data visualisation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Import train_test_split from scikit library\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import Keras\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "epochs = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-36c0d0ae76d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Loading test and train data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/input/fashion-mnist_train.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/input/fashion-mnist_test.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#Loading test and train data\n",
    "train_df = pd.read_csv('/kaggle/input/fashion-mnist_train.csv',sep=',')\n",
    "test_df = pd.read_csv('/kaggle/input/fashion-mnist_test.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "baa8329ad78e6fe6d0cb9a81c7d4d4b19c1d0ca5"
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ba9a7fdb92a7ec9ea46165d29580e91cec7f0532"
   },
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "526bafe18be4e17fbc0fe307006a2205aa1a93e0"
   },
   "source": [
    "Now let us split the train data into x and y arrays where x represents the image data and y represents the labels.\n",
    "\n",
    "To do that we need to convert the dataframes into numpy arrays of float32 type which is the acceptable form for tensorflow and keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3d45b24549df174152de3e7b32d52098091a0506"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_data = np.array(train_df, dtype = 'float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a5609f34959bf650e77a4fdd0f4021919f0a8130"
   },
   "outputs": [],
   "source": [
    "#Similarly let us do the same process for test data\n",
    "\n",
    "test_data = np.array(test_df, dtype='float32')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "853aa4daf322a190818aa51d0441a96e800a51ec"
   },
   "source": [
    "Now let us slice the train arrays into x and y arrays namely x_train,y_train to store all image data and label data respectively. i.e\n",
    "\n",
    "    x_train contains all the rows and all columns except the label column and excluding header info .\n",
    "\n",
    "    y_train contains all the rows and first column and excluding header info .\n",
    "\n",
    "Similarly slice the test arrays into x and y arrays namely x_test,y_test to store all image data and label data respectively. i.e\n",
    "\n",
    "    x_test contains all the rows and all columns except the label column and excluding header info .\n",
    "    y_test contains all the rows and first column and excluding header info .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fbb97e88df53930ca74286c6e544be94cca1262d"
   },
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f7d368cb8071302e9c294e342c9f64c32b306e9b"
   },
   "source": [
    "from above : Since the image data in x_train and x_test is from 0 to 255 , we need to rescale this from 0 to 1.To do this we need to divide the x_train and x_test by 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "41e8809b133177e6d2b1c858f656f53852dc8336"
   },
   "outputs": [],
   "source": [
    "x_train = train_data[:,1:]/255\n",
    "\n",
    "y_train = train_data[:,0]\n",
    "\n",
    "x_test= test_data[:,1:]/255\n",
    "\n",
    "y_test=test_data[:,0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d54a00227d0b57742eb8fa7260b41b974ede1707"
   },
   "source": [
    "Now split the training data into validation and actual training data for training the model and testing it using the validation set. This is achieved using the train_test_split method of scikit learn library.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d104c22823251e5c293b13676006a41c970fb597"
   },
   "outputs": [],
   "source": [
    "x_train,x_validate,y_train,y_validate = train_test_split(x_train,y_train,test_size = 0.2,random_state = 12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "491923eb56727200d77963033c21a077bf66efc4"
   },
   "outputs": [],
   "source": [
    "#Now let us visualise the sample image how it looks like in 28 * 28 pixel size\n",
    "\n",
    "image = x_train[55,:].reshape((28,28))\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "49156dd359e4c4f94c6a1256a7f01b71f5649f53"
   },
   "source": [
    "## Creating convolutional Neural Networks\n",
    "    1.Define the model\n",
    "\n",
    "    2.Compile the model\n",
    "    \n",
    "    3.Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a9d7555318e21064e18e8c20fd60f9671e4a9607"
   },
   "outputs": [],
   "source": [
    "# Defined the shape of the image as 3d with rows and columns and 1 for the 3d visualisation\n",
    "\n",
    "image_rows = 28\n",
    "\n",
    "image_cols = 28\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "image_shape = (image_rows,image_cols,1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "541eee437b3a698b747a7d67daa74074c5f4298b"
   },
   "outputs": [],
   "source": [
    "#formating on the x_train,x_test and x_validate sets.\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0],*image_shape)\n",
    "x_test = x_test.reshape(x_test.shape[0],*image_shape)\n",
    "x_validate = x_validate.reshape(x_validate.shape[0],*image_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "753b4aa6bf227c71618d5c030b5c8c470d36190d"
   },
   "source": [
    "### Defininig the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1a29d9d92b0fd14fbe2be827aaf9806d5b4b7f7e"
   },
   "outputs": [],
   "source": [
    "cnn_model = Sequential([\n",
    "    Conv2D(filters=32,kernel_size=3,activation='relu',input_shape = image_shape),\n",
    "    MaxPooling2D(pool_size=2) ,# down sampling the output instead of 28*28 it is 14*14\n",
    "    Dropout(0.2),\n",
    "    Flatten(), # flatten out the layers\n",
    "    Dense(32,activation='relu'),\n",
    "    Dense(10,activation = 'softmax')\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "800614e5dcffe6e275263d2cdb6a4b115af57235"
   },
   "source": [
    "### Compiling model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "930a82700d5bfc724ea5880c756a3203af059a3c"
   },
   "outputs": [],
   "source": [
    "cnn_model.compile(loss ='sparse_categorical_crossentropy', optimizer=Adam(lr=0.001),metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "69bbe422d0ceadc1ff3bc204d4adaa6e69e37931"
   },
   "source": [
    "### fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d66f4f79a62c45698df129b35cea9130fcdf789e"
   },
   "outputs": [],
   "source": [
    "history = cnn_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=50,\n",
    "    verbose=1,\n",
    "    validation_data=(x_validate,y_validate),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1a3cf181c949b0d82b7cc441bf20dc39eb161f2b"
   },
   "source": [
    "### confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e9ea9a4926cb808468fad96033760e65f9ef78ee"
   },
   "outputs": [],
   "source": [
    "# Look at confusion matrix \n",
    " \n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Predict the values from the validation dataset\n",
    " \n",
    "Y_pred = model.predict(X_val)\n",
    "# Convert predictions classes to one hot vectors \n",
    " \n",
    "Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
    "# Convert validation observations to one hot vectors\n",
    " \n",
    "Y_true = np.argmax(Y_val,axis = 1) \n",
    "# compute the confusion matrix\n",
    " \n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "# plot the confusion matrix\n",
    " \n",
    "plot_confusion_matrix(confusion_mtx, classes = range(10)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "81d0ad275058cf64c6a57cf8f076a439ec5d435b"
   },
   "source": [
    "### Evaluate / score of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a7d5ee43b3f68a5c11af3a321e0f6ee85e1284d1"
   },
   "outputs": [],
   "source": [
    "score = cnn_model.evaluate(x_test,y_test,verbose=0)\n",
    "print('Test Loss : {:.4f}'.format(score[0]))\n",
    "print('Test Accuracy : {:.4f}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c3e8b457e05b78b47400f6d96760c78976475e7a"
   },
   "source": [
    "### plotting the result ie training accuracy vs validation accuracy and training loss and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d2dc15d452cee804ed2027b8f65cdd493ae20144"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "accuracy = history.history['acc']\n",
    "val_accuracy = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(len(accuracy))\n",
    "plt.plot(epochs, accuracy, 'bo', label='Training Accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'b', label='Validation Accuracy')\n",
    "plt.title('Training and Validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b041199ed35333ddddc541796f46ee952ec35148"
   },
   "source": [
    "\n",
    "### summarizing the performance of our classifier as follows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b91f7896d8bd8c5441336ef98ce30750d8877ffc"
   },
   "outputs": [],
   "source": [
    "#get the predictions for the test data\n",
    "predicted_classes = cnn_model.predict_classes(x_test)\n",
    "#get the indices to be plotted\n",
    "y_true = test_df.iloc[:, 0]\n",
    "correct = np.nonzero(predicted_classes==y_true)[0]\n",
    "incorrect = np.nonzero(predicted_classes!=y_true)[0]\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
    "print(classification_report(y_true, predicted_classes, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "355aadb864ac3dde8f570cb8ffa8a9936cbb7252"
   },
   "source": [
    "It's apparent that our classifier is underperforming for class 6 in terms of both precision and recall. For class 2, classifier is slightly lacking precision whereas it is slightly lacking recall (i.e. missed) for class 4.\n",
    "\n",
    "\n",
    "Perhaps we would gain more insight after visualizing the correct and incorrect predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "51648cc452989a3e046a94a6194ecd9bd009c31e"
   },
   "source": [
    "\n",
    "### Subset of correctly predicted classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9ab87b2675b6e83764ab9e3499e7c7d823db351a"
   },
   "outputs": [],
   "source": [
    "for i, correct in enumerate(correct[:9]):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(x_test[correct].reshape(28,28), cmap='gray', interpolation='none')\n",
    "    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[correct], y_true[correct]))\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f395d5c26261146a3b8cfeebf6e21723c7175df4"
   },
   "source": [
    "### Subset of incorrectly predicted classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8416e396167deeb5ce6948038ec82f92e0efa571"
   },
   "outputs": [],
   "source": [
    "\n",
    "for i, incorrect in enumerate(incorrect[0:9]):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(x_test[incorrect].reshape(28,28), cmap='gray', interpolation='none')\n",
    "    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[incorrect], y_true[incorrect]))\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fc7620b512ef52baa094407a1b8b8ac1b85b007e"
   },
   "source": [
    "### Predict and Submit results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e65a21ff2afae6b618f6ff43dc86fa2e882b4281"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# predict results\n",
    " \n",
    "results = model.predict(test)\n",
    "\n",
    "# select the indix with the maximum probability\n",
    " \n",
    "results = np.argmax(results,axis = 1)\n",
    "\n",
    "results = pd.Series(results,name=\"Label\")\n",
    "\n",
    "\n",
    "\n",
    "submission = pd.concat([pd.Series(range(1,10001),name = \"id\"),results],axis = 1)\n",
    "\n",
    "submission.to_csv(\"cnn_mnist_datagen.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "02ad36fdd318f9d91988bd4e7bcc1f90dbf68143"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
