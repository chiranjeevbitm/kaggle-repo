{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "print (cv2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load an color image in grayscale\n",
    "img = cv2.imread('image.jpg',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1aec381e2c12374befd01e0c4f1ba32fb216389d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img = cv2.imread('image.jpg',0)\n",
    "plt.imshow(img, cmap = 'gray', interpolation = 'bicubic')\n",
    "plt.xticks([]), plt.yticks([])  # to hide tick values on X and Y axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dc1ceb3316287f0b7e605806cea0558385ef228d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# capturing from camera\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "cap = cv2.VideoCapture(0)\n",
    "while(True):\n",
    "# Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "# Our operations on the frame come here\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "# Display the resulting frame\n",
    "    cv2.imshow('frame',gray)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "cap = cv2.VideoCapture('video.mp4')\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    cv2.imshow('frame',gray)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Below code capture from a Camera, flip every frame in vertical direction and saves it\n",
    "import numpy as np\n",
    "import cv2\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('output.avi',fourcc, 20.0, (640,480))\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if ret==True:\n",
    "        frame = cv2.flip(frame,0)\n",
    "# write the flipped frame\n",
    "        out.write(frame)\n",
    "        cv2.imshow('frame',frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "# Release everything if job is finished\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drawing a line \n",
    "import numpy as np\n",
    "import cv2\n",
    "# Create a black image\n",
    "img = np.zeros((512,512,3), np.uint8)\n",
    "# Draw a diagonal blue line with thickness of 5 px\n",
    "img = cv2.line(img,(0,0),(511,511),(255,0,0),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "cv2.putText(img,'OpenCV',(10,500), font, 4,(255,255,255),2,cv2.LINE_AA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Trackbar as the Color Palette\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def nothing(x):\n",
    "    pass\n",
    "\n",
    "# Create a black image, a window\n",
    "img = np.zeros((300,512,3), np.uint8)\n",
    "cv2.namedWindow('image')\n",
    "\n",
    "# create trackbars for color change\n",
    "cv2.createTrackbar('R','image',0,255,nothing)\n",
    "cv2.createTrackbar('G','image',0,255,nothing)\n",
    "cv2.createTrackbar('B','image',0,255,nothing)\n",
    "\n",
    "# create switch for ON/OFF functionality\n",
    "switch = '0 : OFF \\n1 : ON'\n",
    "cv2.createTrackbar(switch, 'image',0,1,nothing)\n",
    "\n",
    "while(1):\n",
    "    cv2.imshow('image',img)\n",
    "    k = cv2.waitKey(1) & 0xFF\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "    # get current positions of four trackbars\n",
    "    r = cv2.getTrackbarPos('R','image')\n",
    "    g = cv2.getTrackbarPos('G','image')\n",
    "    b = cv2.getTrackbarPos('B','image')\n",
    "    s = cv2.getTrackbarPos(switch,'image')\n",
    "\n",
    "    if s == 0:\n",
    "        img[:] = 0\n",
    "    else:\n",
    "        img[:] = [b,g,r]\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading watch image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img = cv2.imread('watch.jpg',cv2.IMREAD_GRAYSCALE)\n",
    "cv2.imshow('image',img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open CV "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Loading Images aand videos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using matpotib displaying image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img = cv2.imread('watch.jpg',cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "plt.imshow(img, cmap = 'gray', interpolation = 'bicubic')\n",
    "plt.xticks([]), plt.yticks([])  # to hide tick values on X and Y axis\n",
    "#plt.plot([200,300,400],[100,200,300],'c', linewidth=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saving a new image\n",
    "cv2.imwrite('watchgray.png',img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loading Video Source OpenCV Python\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('output.avi',fourcc, 20.0, (640,480))\n",
    "\n",
    "while(True):\n",
    "    ret, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    out.write(frame)\n",
    "    cv2.imshow('frame',gray)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drawing and Writing on Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drawing and Writing on Image \n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "img = cv2.imread('image.jpg',cv2.IMREAD_COLOR)\n",
    "\n",
    "#Next, we can start drawing, like:\n",
    "\n",
    "cv2.line(img,(0,0),(350,350),(255,255,255),15)\n",
    "cv2.circle(img, (340,340), 50, (255,255,255),15)\n",
    "\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "cv2.putText(img,'Its me!',(10,500), font, 6, (200,255,155), 13, cv2.LINE_AA)\n",
    "\n",
    "cv2.imshow('image',img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# opencv drawing line rectangles and circles and putting texts\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "img = cv2.imread('download.jpg',cv2.IMREAD_COLOR)\n",
    "cv2.line(img,(0,0),(200,300),(255,255,255),50)\n",
    "cv2.rectangle(img,(500,250),(1000,500),(0,0,255),15)\n",
    "cv2.circle(img,(447,63), 63, (0,255,0), -1)\n",
    "pts = np.array([[100,50],[200,300],[700,200],[500,100]], np.int32)\n",
    "pts = pts.reshape((-1,1,2))\n",
    "cv2.polylines(img, [pts], True, (0,255,255), 3)\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "cv2.putText(img,'OpenCV Tuts!',(10,500), font, 6, (200,255,155), 13, cv2.LINE_AA)\n",
    "cv2.imshow('image',img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Image Operations \n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('watch.jpg',cv2.IMREAD_COLOR)\n",
    "#Now, we can reference specific pixels, like so:\n",
    "\n",
    "px = img[55,55]\n",
    "#Next, we could actually change a pixel:\n",
    "\n",
    "img[55,55] = [255,255,255]\n",
    "#Then re-reference:\n",
    "\n",
    "px = img[55,55]\n",
    "print(px)\n",
    "#It should be different now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Next, we can reference an ROI, or Region of Image, like so:\n",
    "\n",
    "px = img[100:150,100:150]\n",
    "print(px)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We can also modify the ROI, like this:\n",
    "\n",
    "img[100:150,100:150] = [255,255,255]\n",
    "#We can reference certain characteristics of our image:\n",
    "\n",
    "print(img.shape)\n",
    "print(img.size)\n",
    "print(img.dtype)\n",
    "#And we can perform operations, like:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "watch_face = img[37:111,107:194]\n",
    "img[0:74,0:87] = watch_face\n",
    "\n",
    "cv2.imshow('image',img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Image arithmetics and Logic operations\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# 500 x 250\n",
    "img1 = cv2.imread('3D-Matplotlib.png')\n",
    "img2 = cv2.imread('mainsvmimage.png')\n",
    "\n",
    "add = img1+img2\n",
    "\n",
    "cv2.imshow('add',add)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "add = cv2.add(img1,img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv2.imshow('add',add)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adding one image to other\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load two images\n",
    "img1 = cv2.imread('3D-Matplotlib.png')\n",
    "img2 = cv2.imread('mainlogo.png')\n",
    "\n",
    "# I want to put logo on top-left corner, So I create a ROI\n",
    "rows,cols,channels = img2.shape\n",
    "roi = img1[0:rows, 0:cols ]\n",
    "\n",
    "# Now create a mask of logo and create its inverse mask\n",
    "img2gray = cv2.cvtColor(img2,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# add a threshold\n",
    "ret, mask = cv2.threshold(img2gray, 220, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "mask_inv = cv2.bitwise_not(mask)\n",
    "\n",
    "# Now black-out the area of logo in ROI\n",
    "img1_bg = cv2.bitwise_and(roi,roi,mask = mask_inv)\n",
    "\n",
    "# Take only region of logo from logo image.\n",
    "img2_fg = cv2.bitwise_and(img2,img2,mask = mask)\n",
    "\n",
    "dst = cv2.add(img1_bg,img2_fg)\n",
    "img1[0:rows, 0:cols ] = dst\n",
    "\n",
    "cv2.imshow('res',img1)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thresholding OpenCV Python Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Thresholding OpenCV Python Tutorial\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('bookpage.jpg')\n",
    "\n",
    "retval, threshold = cv2.threshold(img, 12, 255, cv2.THRESH_BINARY)\n",
    "cv2.imshow('original',img)\n",
    "cv2.imshow('threshold',threshold)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#in grayscaled\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "grayscaled = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "retval, threshold = cv2.threshold(grayscaled, 10, 255, cv2.THRESH_BINARY)\n",
    "cv2.imshow('original',img)\n",
    "cv2.imshow('threshold',threshold)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adaptive threshold\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "th = cv2.adaptiveThreshold(grayscaled, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 115, 1)\n",
    "cv2.imshow('original',img)\n",
    "cv2.imshow('Adaptive threshold',th)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Otsu threshold\n",
    "retval2,threshold2 = cv2.threshold(grayscaled,125,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "cv2.imshow('original',img)\n",
    "cv2.imshow('Otsu threshold',threshold2)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Color filtering with opencv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while(1):\n",
    "    _, frame = cap.read()\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    lower_red = np.array([30,150,50])\n",
    "    upper_red = np.array([255,255,180])\n",
    "    \n",
    "    \n",
    "    mask = cv2.inRange(hsv, lower_red, upper_red)\n",
    "    res = cv2.bitwise_and(frame,frame, mask= mask)\n",
    "\n",
    "    cv2.imshow('frame',frame)\n",
    "    cv2.imshow('mask',mask)\n",
    "    cv2.imshow('res',res)\n",
    "    \n",
    "    k = cv2.waitKey(5) & 0xFF\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blurring and smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while(1):\n",
    "\n",
    "    _, frame = cap.read()\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    lower_red = np.array([30,150,50])\n",
    "    upper_red = np.array([255,255,180])\n",
    "    \n",
    "    mask = cv2.inRange(hsv, lower_red, upper_red)\n",
    "    res = cv2.bitwise_and(frame,frame, mask= mask)\n",
    "    kernel = np.ones((15,15),np.float32)/225\n",
    "    smoothed = cv2.filter2D(res,-1,kernel)\n",
    "    cv2.imshow('Original',frame)\n",
    "    cv2.imshow('Averaging',smoothed)\n",
    "    # option2\n",
    "    blur = cv2.GaussianBlur(res,(15,15),0)\n",
    "    cv2.imshow('Gaussian Blurring',blur)\n",
    "    #option 3\n",
    "    bilateral = cv2.bilateralFilter(res,15,75,75)\n",
    "    cv2.imshow('bilateral Blur',bilateral)\n",
    "\n",
    "    k = cv2.waitKey(5) & 0xFF\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morphological transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while(1):\n",
    "\n",
    "    _, frame = cap.read()\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    lower_red = np.array([30,150,50])\n",
    "    upper_red = np.array([255,255,180])\n",
    "    \n",
    "    mask = cv2.inRange(hsv, lower_red, upper_red)\n",
    "    res = cv2.bitwise_and(frame,frame, mask= mask)\n",
    "\n",
    "    kernel = np.ones((5,5),np.uint8)\n",
    "    erosion = cv2.erode(mask,kernel,iterations = 1)\n",
    "    dilation = cv2.dilate(mask,kernel,iterations = 1)\n",
    "\n",
    "    cv2.imshow('Original',frame)\n",
    "    cv2.imshow('Mask',mask)\n",
    "    cv2.imshow('Erosion',erosion)\n",
    "    cv2.imshow('Dilation',dilation)\n",
    "\n",
    "    k = cv2.waitKey(5) & 0xFF\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while(1):\n",
    "\n",
    "    _, frame = cap.read()\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    lower_red = np.array([30,150,50])\n",
    "    upper_red = np.array([255,255,180])\n",
    "    \n",
    "    mask = cv2.inRange(hsv, lower_red, upper_red)\n",
    "    res = cv2.bitwise_and(frame,frame, mask= mask)\n",
    "\n",
    "    kernel = np.ones((5,5),np.uint8)\n",
    "    \n",
    "    opening = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "    closing = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    cv2.imshow('Original',frame)\n",
    "    cv2.imshow('Mask',mask)\n",
    "    cv2.imshow('Opening',opening)\n",
    "    cv2.imshow('Closing',closing)\n",
    "\n",
    "    k = cv2.waitKey(5) & 0xFF\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Canny Edge Detection and Gradients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "while(1):\n",
    "\n",
    "    # Take each frame\n",
    "    _, frame = cap.read()\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    lower_red = np.array([30,150,50])\n",
    "    upper_red = np.array([255,255,180])\n",
    "    \n",
    "    mask = cv2.inRange(hsv, lower_red, upper_red)\n",
    "    res = cv2.bitwise_and(frame,frame, mask= mask)\n",
    "\n",
    "    laplacian = cv2.Laplacian(frame,cv2.CV_64F)\n",
    "    sobelx = cv2.Sobel(frame,cv2.CV_64F,1,0,ksize=5)\n",
    "    sobely = cv2.Sobel(frame,cv2.CV_64F,0,1,ksize=5)\n",
    "\n",
    "    cv2.imshow('Original',frame)\n",
    "    cv2.imshow('Mask',mask)\n",
    "    cv2.imshow('laplacian',laplacian)\n",
    "    cv2.imshow('sobelx',sobelx)\n",
    "    cv2.imshow('sobely',sobely)\n",
    "\n",
    "    k = cv2.waitKey(5) & 0xFF\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#While we can use these gradients to convert to pure edges, we can also use Canny Edge detection!\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while(1):\n",
    "\n",
    "    _, frame = cap.read()\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    lower_red = np.array([30,150,50])\n",
    "    upper_red = np.array([255,255,180])\n",
    "    \n",
    "    mask = cv2.inRange(hsv, lower_red, upper_red)\n",
    "    res = cv2.bitwise_and(frame,frame, mask= mask)\n",
    "\n",
    "    cv2.imshow('Original',frame)\n",
    "    edges = cv2.Canny(frame,100,200)\n",
    "    cv2.imshow('Edges',edges)\n",
    "\n",
    "    k = cv2.waitKey(5) & 0xFF\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template Matching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load in both images, convert to gray. We keep the original RGB image, and create a grayscale version.\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img_rgb = cv2.imread('opencv-template-matching-python-tutorial.jpg')\n",
    "img_gray = cv2.cvtColor(img_rgb, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "template = cv2.imread('opencv-template-for-matching.jpg',0)\n",
    "w, h = template.shape[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = cv2.matchTemplate(img_gray,template,cv2.TM_CCOEFF_NORMED)\n",
    "threshold = 0.8\n",
    "loc = np.where( res >= threshold)\n",
    "\n",
    "\n",
    "#Here, we call res the matchTemplate between the img_gray (our main image), the template, and then the matching method\n",
    "#we're going to use. We specify a threshold, here 0.8 for 80%. Then we find locations with a logical\n",
    "#statement, where the res is greater than or equal to 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we mark all matches on the original image, using the coordinates we found in the gray image:\n",
    "\n",
    "for pt in zip(*loc[::-1]):\n",
    "    cv2.rectangle(img_rgb, pt, (pt[0] + w, pt[1] + h), (0,255,255), 2)\n",
    "\n",
    "cv2.imshow('Detected',img_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GrabCut Foreground Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img = cv2.imread('image1.jpg')\n",
    "mask = np.zeros(img.shape[:2],np.uint8)\n",
    "\n",
    "bgdModel = np.zeros((1,65),np.float64)\n",
    "fgdModel = np.zeros((1,65),np.float64)\n",
    "\n",
    "rect = (161,79,150,150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.grabCut(img,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)\n",
    "mask2 = np.where((mask==2)|(mask==0),0,1).astype('uint8')\n",
    "img = img*mask2[:,:,np.newaxis]\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corner Detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "img = cv2.imread('opencv-corner-detection-sample.jpg')\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "gray = np.float32(gray)\n",
    "\n",
    "corners = cv2.goodFeaturesToTrack(gray, 100, 0.01, 10)\n",
    "corners = np.int0(corners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for corner in corners:\n",
    "    x,y = corner.ravel()\n",
    "    cv2.circle(img,(x,y),3,255,-1)\n",
    "    \n",
    "cv2.imshow('Corner',img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Feature Matching (Homography) Brute Force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img1 = cv2.imread('opencv-feature-matching-template.jpg',0)\n",
    "img2 = cv2.imread('opencv-feature-matching-image.jpg',0)\n",
    "\n",
    "#So far we've imported the modules we're going to use, and defined our two images,\n",
    "#the template (img1) and the image we're going to search for the template in (img2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orb = cv2.ORB_create()\n",
    "\n",
    "#This is the detector we're going to use for the features.\n",
    "\n",
    "kp1, des1 = orb.detectAndCompute(img1,None)\n",
    "kp2, des2 = orb.detectAndCompute(img2,None)\n",
    "\n",
    "#Here, we find the key points and their descriptors with the orb detector.\n",
    "\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This is our BFMatcher object.\n",
    "\n",
    "matches = bf.match(des1,des2)\n",
    "matches = sorted(matches, key = lambda x:x.distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we create matches of the descriptors, then we sort them based on their distances.\n",
    "\n",
    "img3 = cv2.drawMatches(img1,kp1,img2,kp2,matches[:10],None, flags=2)\n",
    "plt.imshow(img3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOG Background Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture('people-walking.mp4')\n",
    "fgbg = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "while(1):\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    fgmask = fgbg.apply(frame)\n",
    " \n",
    "    cv2.imshow('fgmask',frame)\n",
    "    cv2.imshow('frame',fgmask)\n",
    "\n",
    "    \n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "    \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Haar Cascade Object Detection Face & Eye  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# multiple cascades: https://github.com/Itseez/opencv/tree/master/data/haarcascades\n",
    "\n",
    "#https://github.com/Itseez/opencv/blob/master/data/haarcascades/haarcascade_frontalface_default.xml\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "#https://github.com/Itseez/opencv/blob/master/data/haarcascades/haarcascade_eye.xml\n",
    "eye_cascade = cv2.CascadeClassifier('haarcascade_eye.xml')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Here, we begin with import cv2 and numpy, then we load in our face and eye cascades. Simple enough so far.\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while 1:\n",
    "    ret, img = cap.read()\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    \n",
    "    \n",
    "#Now we begin our typical loop, the only new thing here is the creation of faces.\n",
    "# Basically, it finds faces! We also want to find eyes,\n",
    " \n",
    "#but, in a world of false positives, wouldn't it be prudent to logically make it so that we only find eyes in faces\n",
    "\n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = img[y:y+h, x:x+w]\n",
    "        \n",
    "        \n",
    "#Here, we're finding faces, their sizes, drawing rectangles, and noting the ROI.\n",
    "# Next, we poke around for some eyes:\n",
    "# If we find those, we'll go ahead and make some more rectangles. Next we finish up:\n",
    "\n",
    "\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "        for (ex,ey,ew,eh) in eyes:\n",
    "            cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
    "\n",
    "    cv2.imshow('img',img)\n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating own haar Cascade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def store_raw_images():\n",
    "    neg_images_link = '//image-net.org/api/text/imagenet.synset.geturls?wnid=n00523513'   \n",
    "    neg_image_urls = urllib.request.urlopen(neg_images_link).read().decode()\n",
    "    pic_num = 1\n",
    "    \n",
    "    if not os.path.exists('neg'):\n",
    "        os.makedirs('neg')\n",
    "        \n",
    "    for i in neg_image_urls.split('\\n'):\n",
    "        try:\n",
    "            print(i)\n",
    "            urllib.request.urlretrieve(i, \"neg/\"+str(pic_num)+\".jpg\")\n",
    "            img = cv2.imread(\"neg/\"+str(pic_num)+\".jpg\",cv2.IMREAD_GRAYSCALE)\n",
    "            # should be larger than samples / pos pic (so we can place our image on it)\n",
    "            resized_image = cv2.resize(img, (100, 100))\n",
    "            cv2.imwrite(\"neg/\"+str(pic_num)+\".jpg\",resized_image)\n",
    "            pic_num += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(str(e))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_uglies():\n",
    "    match = False\n",
    "    for file_type in ['neg']:\n",
    "        for img in os.listdir(file_type):\n",
    "            for ugly in os.listdir('uglies'):\n",
    "                try:\n",
    "                    current_image_path = str(file_type)+'/'+str(img)\n",
    "                    ugly = cv2.imread('uglies/'+str(ugly))\n",
    "                    question = cv2.imread(current_image_path)\n",
    "                    if ugly.shape == question.shape and not(np.bitwise_xor(ugly,question).any()):\n",
    "                        print('That is one ugly pic! Deleting!')\n",
    "                        print(current_image_path)\n",
    "                        os.remove(current_image_path)\n",
    "                except Exception as e:\n",
    "                    print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def store_raw_images():\n",
    "    neg_images_link = '//image-net.org/api/text/imagenet.synset.geturls?wnid=n07942152'   \n",
    "    neg_image_urls = urllib.request.urlopen(neg_images_link).read().decode()\n",
    "    pic_num = 953\n",
    "    \n",
    "    if not os.path.exists('neg'):\n",
    "        os.makedirs('neg')\n",
    "        \n",
    "    for i in neg_image_urls.split('\\n'):\n",
    "        try:\n",
    "            print(i)\n",
    "            urllib.request.urlretrieve(i, \"neg/\"+str(pic_num)+\".jpg\")\n",
    "            img = cv2.imread(\"neg/\"+str(pic_num)+\".jpg\",cv2.IMREAD_GRAYSCALE)\n",
    "            # should be larger than samples / pos pic (so we can place our image on it)\n",
    "            resized_image = cv2.resize(img, (100, 100))\n",
    "            cv2.imwrite(\"neg/\"+str(pic_num)+\".jpg\",resized_image)\n",
    "            pic_num += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(str(e)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_pos_n_neg():\n",
    "    for file_type in ['neg']:\n",
    "        \n",
    "        for img in os.listdir(file_type):\n",
    "\n",
    "            if file_type == 'pos':\n",
    "                line = file_type+'/'+img+' 1 0 0 50 50\\n'\n",
    "                with open('info.dat','a') as f:\n",
    "                    f.write(line)\n",
    "            elif file_type == 'neg':\n",
    "                line = file_type+'/'+img+'\\n'\n",
    "                with open('bg.txt','a') as f:\n",
    "                    f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('haarcascade_eye.xml')\n",
    "\n",
    "#this is the cascade we just made. Call what you want\n",
    "watch_cascade = cv2.CascadeClassifier('watchcascade10stage.xml')\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while 1:\n",
    "    ret, img = cap.read()\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    \n",
    "    # add this\n",
    "    # image, reject levels level weights.\n",
    "    watches = watch_cascade.detectMultiScale(gray, 50, 50)\n",
    "    \n",
    "    # add this\n",
    "    for (x,y,w,h) in watches:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,255,0),2)\n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "\n",
    "        \n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = img[y:y+h, x:x+w]\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "        for (ex,ey,ew,eh) in eyes:\n",
    "            cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
    "\n",
    "    cv2.imshow('img',img)\n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
